

















@inbook{doi:10.1002/9781118557426.ch9,
author = {Beynier, Aurélie and Charpillet, François and Szer, Daniel and Mouaddib, Abdel-Illah},
publisher = {John Wiley & Sons, Ltd},
isbn = {9781118557426},
title = {DEC-MDP/POMDP},
booktitle = {Markov Decision Processes in Artificial Intelligence},
chapter = {9},
pages = {277-318},
doi = {10.1002/9781118557426.ch9},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118557426.ch9},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118557426.ch9},
year = {2013},
keywords = {DEC-POMDPs, decentralized Markov decision processes (DEC-MDPs), multiagent decentralized control, multiagent decision problems},
abstract = {Summary Markov decision processes (MDPs) and partially observable Markov decision processes (DEC-POMDPs) are both mathematical models that have been successfully used to formalize sequential decision-theoretic problems under uncertainty. These models rely on different types of hypotheses that can be classified within: i) each agent has a complete knowledge of the system state; ii) each agent has a partial knowledge of the system state; iii) the agents can communicate; iv) the agents cannot communicate. These hypotheses have led to several formalisms. Among them, this chapter reviews the most well-known ones: MMDP, Decentralized MDPs (DEC-MDPs), Decentralized POMDPs (DEC-POMDPs), MTDP, Dec-MDP-Com, COM-MTDP, ND-POMDP, TI-Dec-MDP, OC-Dec-MDP. It also deals with the complexity of computing optimal solutions for the multiagent decision problems described with these formalisms. DEC-POMDPs and DEC-MDPs extend POMDPs and MDPs to multiagent decentralized control. Controlled Vocabulary Terms multi-agent systems; uncertainty handling}
}
